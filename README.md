---
title: LLMOps RAG Solution HS Spaces
emoji: ðŸ§ 
colorFrom: blue
colorTo: red
sdk: docker
app_file: Dockerfile
pinned: false
license: mit
---

# LLMOps / RAG Solution

This repository supports **two parallel execution modes** for the same application:

1. **Local development via Docker Compose (http://localhost:8000/)**
2. **Production-style deployment on Hugging Face Spaces (https://huggingface.co/spaces/LeonardoMdSA/LLMOps-RAG_solution-HS_spaces)**

Both modes run the application inside containers, but they are intentionally **separated** to avoid coupling local tooling with Hugging Face constraints.

---

## Repository Structure

```text
LLMOps-RAG_solution_HS_spaces/
â”œâ”€â”€ app.py
â”‚   # Main FastAPI application entrypoint.
â”œâ”€â”€ docker-compose.yml
â”‚   # Optional local orchestration file.
â”œâ”€â”€ Dockerfile
â”‚   # Production container definition used by Hugging Face Spaces.
â”œâ”€â”€ LICENSE
â”‚   # Repository license file.
â”œâ”€â”€ pytest.ini
â”‚   # Pytest configuration.
â”œâ”€â”€ README.md
â”‚   # Project documentation.
â”œâ”€â”€ requirements.txt
â”‚   # Python runtime dependencies.
â”œâ”€â”€ faiss_index/
â”‚   # Runtime-generated FAISS vector index directory.
â”œâ”€â”€ localhost/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   # Alternative lightweight Dockerfile for local-only usage.
â”‚   â”‚   # Decoupled from Hugging Face Spaces constraints.
â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   # Local development entrypoint.
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   # Local Python project configuration.
â”‚   â””â”€â”€ requirements.txt
â”‚       # Local-only dependency set
â”œâ”€â”€ models/
â”‚   â””â”€â”€ qwen2.5-0.5b-instruct-q4_0.gguf
â”‚       # Quantized local LLM model file.
â”œâ”€â”€ multi_doc_chat/
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   # Responsible for loading the local LLM model.
â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   # Core RAG orchestration layer.
â”‚   â”œâ”€â”€ src/
â”‚   â”‚    â””â”€â”€ document_ingestion/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   # Marks ingestion as a Python module.
â”‚   â”‚       â””â”€â”€ data_ingestion.py
â”‚   â”‚           # Document ingestion pipeline.
â”‚   â””â”€â”€ utils/
â”‚         â”œâ”€â”€ __init__.py
â”‚         â”‚   # Utility module marker.
â”‚         â”‚
â”‚         â””â”€â”€ document_ops.py
â”‚             # Low-level document utilities.
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_models.py
â”‚       # Bootstrap script for model availability.
â”œâ”€â”€ static/
â”‚   â””â”€â”€ styles.css
â”‚       # Frontend styling for the web UI.
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”‚       # Static HTML frontend.
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”‚   # Test package marker.
    â”‚
    â”œâ”€â”€ run_evaluations.py
    â”‚   # Manual or scripted evaluation runner.
    â”‚
    â”œâ”€â”€ integration/
    â”‚   â””â”€â”€ test_rag_service_flow.py
    â”‚       # End-to-end integration tests.
    â”‚
    â””â”€â”€ unit/
        â””â”€â”€ test_data_ingestion.py
            # Unit tests for the ingestion layer.
```

### Key Design Decision

* **`localhost/`** is exclusively for local development and testing
* **Repository root** is optimized for **Hugging Face Spaces**

They serve different runtimes and must not be merged.

---

## Local Development (Docker Compose)

The `localhost/` folder allows you to run the full application locally inside Docker using **docker-compose**.

### Characteristics

* Uses `docker-compose.yml`
* Can spin up multiple services (API, vector DB, tools, etc.)
* Suitable for:

  * Rapid iteration
  * Debugging
  * Volume mounting
  * Local experimentation

### Run Locally

```bash
docker-compose up --build
```

This mode is **not** used by Hugging Face Spaces and is ignored during HF deployment.

---

## Hugging Face Spaces Deployment (Docker)

Hugging Face Spaces builds and runs the application using **only the repository root**.

### Required Files (Root)

* `app.py` â†’ **mandatory entrypoint**
* `Dockerfile` â†’ defines the HF Space image
* `requirements.txt` â†’ Python dependencies

HF Spaces:

* Does **not** use `docker-compose`
* Builds a **single container**
* Exposes the app based on what `app.py` launches

### Deployment Flow

1. Push repository to Hugging Face Spaces
2. HF detects `Dockerfile`
3. Image is built automatically
4. `app.py` is executed inside the container

No local-only files are required or read.

---

## Testing

This repository includes a dedicated `tests/` folder for automated testing.

### Test Structure

```text
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_*.py
â””â”€â”€ ... (unit and integration tests)
```

Tests are written using **pytest** and are designed to validate:

* Core application logic
* RAG / retrieval components
* Data processing and utility functions

### Run Tests Locally

From the repository root:

```bash
pytest -v
```

Or explicitly:

```bash
python -m pytest
```

Tests are intended to be executed **locally or in CI**. They are not required for Hugging Face Spaces runtime execution and are not invoked during HF builds unless you explicitly add them to the Dockerfile.

---

## Technology Stack

This project implements a production-style **Retrieval-Augmented Generation (RAG)** system using a fully custom pipeline (no LangChain).

---

### Language & Runtime
- **Python 3.13**
  - Local development
  - Hugging Face Spaces runtime compatibility

---

### API & Web Server
- **FastAPI**
  - REST API for document upload and chat
  - Async request handling
- **Uvicorn**
  - ASGI server
  - Configured for port `7860` (HF Spaces requirement)

---

### Frontend
- **HTML / CSS**
- **Jinja2 Templates**
- **FastAPI StaticFiles**
- No Streamlit
- No JavaScript framework (React/Vue)

---

### Retrieval-Augmented Generation (RAG)

#### Embeddings
- **sentence-transformers**
  - Model: `sentence-transformers/all-MiniLM-L6-v2`
  - Used exclusively for document embeddings
  - Loaded at application startup

#### Vector Store
- **FAISS**
  - In-memory vector index with filesystem persistence
  - Index directory: `faiss_index/`
  - Ephemeral on Hugging Face Spaces (resets when container sleeps)

#### Retrieval Logic
- Fully custom implementation
- Manual chunking, embedding, indexing, and similarity search
- No LangChain or LlamaIndex

---

### Large Language Model (LLM)

- **GGUF-format local LLM**
  - Example: `Qwen2.5-0.5B-Instruct (Q4_0)`
- **llama.cpp-compatible runtime**
- Models downloaded at startup into `models/`
- Used for generation only (not embeddings)

---

### Document Ingestion

- **Supported formats**
  - `.txt`
  - `.pdf`
- **Libraries**
  - `PyPDF2` (functional; noted as deprecated)
- Async ingestion pipeline
- Chunking and embedding performed during upload

---

### Deployment
- **Hugging Face Spaces (Docker-based)**
  - Stateless container
  - No persistent volume
  - FAISS index and uploaded documents reset when container sleeps
- Local Docker-compatible structure

---

### Testing
- **pytest**
- **pytest-asyncio**
- Unit tests
- Integration tests
- External dependencies (FAISS, embedder) mocked where required

---

### Utilities & Tooling
- **requests** â€“ model downloads
- **tqdm** â€“ download progress visualization
- **logging (stdlib)** â€“ centralized logging in `app.py`
- **threading / uuid** â€“ session and state management

---

### Explicitly Not Used
- LangChain
- LlamaIndex
- Streamlit
- Cloud-hosted LLM APIs
- Managed vector databases (Pinecone, Weaviate, etc.)
- Persistent storage

---

## Important Notes

* Keep **HF-specific files at repository root**
* Keep **local-only tooling inside `localhost/`**
* Do not move `app.py` or root `Dockerfile`
* Do not introduce `docker-compose.yml` at root

This separation is intentional and correct.

---

## References / Documentation

- **FastAPI**  
  https://fastapi.tiangolo.com/  
  API framework.

- **Uvicorn**  
  https://www.uvicorn.org/  
  ASGI server.

- **FAISS**  
  https://github.com/facebookresearch/faiss  
  Vector similarity search and indexing.

- **Sentence-Transformers**  
  https://www.sbert.net/  
  Document embedding generation.

- **Qwen 2.5 (GGUF, quantized)**  
  https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF  
  Local LLM for answer generation.

- **Hugging Face Spaces**  
  https://huggingface.co/docs/spaces/index  
  Deployment platform (Docker-based, ephemeral storage).

- **Docker**  
  https://docs.docker.com/  
  Containerization.

- **Pytest**  
  https://docs.pytest.org/  
  Testing framework.

- **Pydantic**  
  https://docs.pydantic.dev/  
  Request/response validation.

- **Retrieval-Augmented Generation (RAG)**  
  https://arxiv.org/abs/2005.11401  
  System architecture pattern.

---

## Contact / Author

* Hugging Face: [https://huggingface.co/LeonardoMdSA](https://huggingface.co/LeonardoMdSA)
* GitHub: [https://github.com/LeonardoMdSACode](https://github.com/LeonardoMdSACode)

---

## MIT License

This project is licensed under the MIT License. See the `LICENSE` file for details.
